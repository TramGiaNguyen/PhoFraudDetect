# Import cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t
import pandas as pd
import re
import numpy as np
from transformers import AutoTokenizer
from datasets import Dataset, DatasetDict, ClassLabel
from sklearn.model_selection import train_test_split
from functools import lru_cache  # ThÃªm cache decorator
import torch  # Äá»ƒ kiá»ƒm tra device náº¿u cáº§n
import warnings  # Äá»ƒ warn náº¿u sequence dÃ i
import gc  # Garbage collector Ä‘á»ƒ giáº£i phÃ³ng bá»™ nhá»›
import psutil  # Äá»ƒ theo dÃµi sá»­ dá»¥ng RAM
import os

# Load PhoBERT tokenizer (large Ä‘á»ƒ max_length=256 theo giá»›i háº¡n position embeddings)
tokenizer = AutoTokenizer.from_pretrained("vinai/phobert-large")

# Äá»‹nh nghÄ©a max_length phÃ¹ há»£p cho PhoBERT-large
MAX_LENGTH = 256
SLIDING_STRIDE = 64

# Cáº¥u hÃ¬nh tiáº¿t kiá»‡m RAM
BATCH_SIZE = 16  # TÄƒng batch size Ä‘á»ƒ táº­n dá»¥ng vectorization
CHUNK_SIZE = 1000  # TÄƒng chunk size Ä‘á»ƒ giáº£m sá»‘ láº§n xá»­ lÃ½ I/O
MAX_MEMORY_MB = 4000  # TÄƒng giá»›i háº¡n RAM Ä‘á»ƒ táº­n dá»¥ng hiá»‡u quáº£ hÆ¡n
PREFETCH_SIZE = 2  # Sá»‘ lÆ°á»£ng batch Ä‘á»ƒ prefetch

# Keywords má»Ÿ rá»™ng chi tiáº¿t dá»±a trÃªn phÃ¢n loáº¡i lá»«a Ä‘áº£o thá»±c táº¿
keywords = [
    # === OTP VÃ€ XÃC THá»°C ===
    "otp", "mÃ£ otp", "mÃ£ xÃ¡c thá»±c", "mÃ£ báº£o máº­t", "mÃ£ xÃ¡c nháº­n", "gá»­i otp", "nháº­p otp", "xÃ¡c thá»±c otp",
    "mÃ£ báº£o máº­t", "mÃ£ xÃ¡c minh", "mÃ£ báº£o vá»‡", "mÃ£ khÃ´i phá»¥c", "mÃ£ Ä‘Äƒng nháº­p", "mÃ£ xÃ¡c minh danh tÃ­nh",
    "mÃ£ báº£o máº­t 2fa", "mÃ£ xÃ¡c thá»±c 2 lá»›p", "mÃ£ báº£o vá»‡ tÃ i khoáº£n", "mÃ£ xÃ¡c minh giao dá»‹ch", "mÃ£ báº£o máº­t giao dá»‹ch",
    "mÃ£ xÃ¡c nháº­n chuyá»ƒn tiá»n", "mÃ£ báº£o vá»‡ chuyá»ƒn tiá»n", "mÃ£ xÃ¡c minh rÃºt tiá»n", "mÃ£ báº£o máº­t rÃºt tiá»n",
    "mÃ£ xÃ¡c thá»±c Ä‘Äƒng nháº­p", "mÃ£ báº£o vá»‡ Ä‘Äƒng nháº­p", "mÃ£ xÃ¡c minh Ä‘Äƒng kÃ½", "mÃ£ báº£o máº­t Ä‘Äƒng kÃ½",
    "mÃ£ xÃ¡c nháº­n thay Ä‘á»•i thÃ´ng tin", "mÃ£ báº£o vá»‡ thay Ä‘á»•i thÃ´ng tin", "mÃ£ xÃ¡c minh khÃ´i phá»¥c", "mÃ£ báº£o máº­t khÃ´i phá»¥c"
    
    # === CHUYá»‚N TIá»€N VÃ€ TÃ€I CHÃNH ===
    "chuyá»ƒn tiá»n", "chuyá»ƒn ngay", "chuyá»ƒn gáº¥p", "chuyá»ƒn kháº©n cáº¥p", "ná»™p phÃ­", "Ä‘Ã³ng phÃ­", "tráº£ phÃ­",
    "chuyá»ƒn nháº§m tiá»n", "chuyá»ƒn sai tÃ i khoáº£n", "chuyá»ƒn nháº§m sá»‘ tiá»n", "chuyá»ƒn nháº§m ngÃ¢n hÃ ng",
    "stk", "sá»‘ tÃ i khoáº£n", "tÃ i khoáº£n ngÃ¢n hÃ ng", "visa", "mastercard", "tháº» tÃ­n dá»¥ng", "tháº» ghi ná»£",
    "tháº» atm", "tháº» debit", "tháº» prepaid", "tháº» quá»‘c táº¿", "tháº» ná»™i Ä‘á»‹a", "tháº» chip", "tháº» tá»«",
    "wallet", "vÃ­ Ä‘iá»‡n tá»­", "momo", "zalo pay", "vnpay", "airpay", "grabpay", "paypal", "wechat pay",
    "shinhan pay", "kakao pay", "line pay", "apple pay", "google pay", "samsung pay", "huawei pay",
    "hoÃ n tiá»n", "hoÃ n vá»‘n", "hoÃ n thuáº¿", "hoÃ n phÃ­", "hoÃ n phÃ­ dá»‹ch vá»¥", "hoÃ n phÃ­ giao dá»‹ch",
    "tiá»n máº·t", "vÃ ng", "forex", "chá»©ng khoÃ¡n", "cá»• phiáº¿u", "trÃ¡i phiáº¿u", "quá»¹ Ä‘áº§u tÆ°", "báº£o hiá»ƒm nhÃ¢n thá»"
    
    # === QR VÃ€ LINK ===
    "quÃ©t qr", "scan qr", "mÃ£ qr", "click link", "nháº¥p link", "truy cáº­p link", "Ä‘Æ°á»ng link",
    "nháº¥p vÃ o link", "báº¥m vÃ o link", "má»Ÿ link", "vÃ o link", "truy cáº­p website", "vÃ o website",
    "url", "website", "trang web", "trang máº¡ng", "site", "web", "app", "á»©ng dá»¥ng", "táº£i app", "cÃ i Ä‘áº·t app",
    "á»©ng dá»¥ng giáº£", "app giáº£", "pháº§n má»m giáº£", "táº£i pháº§n má»m", "cÃ i Ä‘áº·t pháº§n má»m", "táº£i game",
    "cÃ i Ä‘áº·t game", "táº£i á»©ng dá»¥ng", "cÃ i Ä‘áº·t á»©ng dá»¥ng", "táº£i tool", "cÃ i Ä‘áº·t tool", "táº£i driver",
    "cÃ i Ä‘áº·t driver", "táº£i plugin", "cÃ i Ä‘áº·t plugin", "táº£i extension", "cÃ i Ä‘áº·t extension"
    
    # === TÃŒNH HUá»NG KHáº¨N Cáº¤P ===
    "gáº¥p", "kháº©n cáº¥p", "nguy hiá»ƒm", "cáº¥p cá»©u", "tÃ¬nh huá»‘ng kháº©n", "váº¥n Ä‘á» nghiÃªm trá»ng",
    "váº¥n Ä‘á» cáº¥p bÃ¡ch", "tÃ¬nh huá»‘ng nguy hiá»ƒm", "sá»± cá»‘ kháº©n cáº¥p", "trÆ°á»ng há»£p kháº©n cáº¥p",
    "lá»—i há»‡ thá»‘ng", "sá»± cá»‘", "trá»¥c tráº·c", "báº£o trÃ¬", "nÃ¢ng cáº¥p há»‡ thá»‘ng", "cáº­n táº¿t",
    "cuá»‘i nÄƒm", "cuá»‘i thÃ¡ng", "cuá»‘i tuáº§n", "cuá»‘i ngÃ y", "háº¿t háº¡n", "sáº¯p háº¿t háº¡n",
    "háº¡n chÃ³t", "deadline", "thá»i háº¡n cuá»‘i", "ká»³ háº¡n cuá»‘i", "ngÃ y cuá»‘i", "giá» cuá»‘i"
    
    # === Lá»ªA Äáº¢O TÃ€I CHÃNH ===
    "trÃºng thÆ°á»Ÿng", "nháº­n thÆ°á»Ÿng", "giáº£i thÆ°á»Ÿng", "tiá»n thÆ°á»Ÿng", "quÃ  táº·ng", "táº·ng quÃ ",
    "trÃºng giáº£i", "nháº­n giáº£i", "giáº£i Ä‘áº·c biá»‡t", "giáº£i nháº¥t", "giáº£i nhÃ¬", "giáº£i ba", "giáº£i khuyáº¿n khÃ­ch",
    "Ä‘áº§u tÆ°", "Ä‘áº§u tÆ° sinh lá»i", "lÃ£i suáº¥t cao", "nhÃ¢n Ä‘Ã´i tiá»n", "gáº¥p Ä‘Ã´i", "lá»£i nhuáº­n",
    "lÃ£i suáº¥t Æ°u Ä‘Ã£i", "lÃ£i suáº¥t Ä‘áº·c biá»‡t", "lÃ£i suáº¥t khá»§ng", "lÃ£i suáº¥t cao nháº¥t", "lÃ£i suáº¥t tá»‘t nháº¥t",
    "cÆ¡ há»™i kiáº¿m tiá»n", "kiáº¿m tiá»n online", "cÃ´ng viá»‡c online", "part time", "lÃ m giÃ u nhanh",
    "lÃ m giÃ u trong 1 thÃ¡ng", "lÃ m giÃ u trong 1 tuáº§n", "lÃ m giÃ u trong 1 ngÃ y", "kiáº¿m tiá»n dá»… dÃ ng",
    "kiáº¿m tiá»n khÃ´ng cáº§n vá»‘n", "kiáº¿m tiá»n khÃ´ng cáº§n kinh nghiá»‡m", "kiáº¿m tiá»n khÃ´ng cáº§n báº±ng cáº¥p",
    "Ä‘áº§u tÆ° báº¥t Ä‘á»™ng sáº£n", "Ä‘áº§u tÆ° tiá»n Ä‘iá»‡n tá»­", "Ä‘áº§u tÆ° vÃ ng", "Ä‘áº§u tÆ° bitcoin", "Ä‘áº§u tÆ° ethereum",
    "Ä‘áº§u tÆ° crypto", "Ä‘áº§u tÆ° coin", "Ä‘áº§u tÆ° token", "Ä‘áº§u tÆ° nft", "Ä‘áº§u tÆ° metaverse",
    "Ä‘a cáº¥p", "bÃ¡n hÃ ng Ä‘a cáº¥p", "kinh doanh Ä‘a cáº¥p", "marketing Ä‘a cáº¥p", "bÃ¡n hÃ ng trá»±c tuyáº¿n Ä‘a cáº¥p"
    
    # === Máº O DANH VÃ€ GIáº¢ Máº O ===
    "máº¡o danh", "giáº£ máº¡o", "giáº£ vá»", "giáº£ bá»™", "giáº£ danh", "máº¡o nháº­n", "giáº£ nháº­n",
    "ngÃ¢n hÃ ng", "ngÃ¢n hÃ ng nhÃ  nÆ°á»›c", "ngÃ¢n hÃ ng thÆ°Æ¡ng máº¡i", "ngÃ¢n hÃ ng quá»‘c táº¿", "ngÃ¢n hÃ ng nÆ°á»›c ngoÃ i",
    "cÃ´ng an", "cáº£nh sÃ¡t", "cáº£nh sÃ¡t giao thÃ´ng", "cáº£nh sÃ¡t hÃ¬nh sá»±", "cáº£nh sÃ¡t kinh táº¿", "cáº£nh sÃ¡t mÃ´i trÆ°á»ng",
    "cÆ¡ quan nhÃ  nÆ°á»›c", "chÃ­nh phá»§", "bá»™ tÃ i chÃ­nh", "bá»™ cÃ´ng an", "bá»™ quá»‘c phÃ²ng", "bá»™ ngoáº¡i giao",
    "cÃ´ng ty", "táº­p Ä‘oÃ n", "cÃ´ng ty Ä‘a quá»‘c gia", "cÃ´ng ty quá»‘c táº¿", "cÃ´ng ty nÆ°á»›c ngoÃ i",
    "nhÃ¢n viÃªn", "cÃ¡n bá»™", "quáº£n lÃ½", "giÃ¡m Ä‘á»‘c", "phÃ³ giÃ¡m Ä‘á»‘c", "trÆ°á»Ÿng phÃ²ng", "phÃ³ phÃ²ng",
    "nhÃ¢n viÃªn báº£o hiá»ƒm", "nhÃ¢n viÃªn ngÃ¢n hÃ ng", "nhÃ¢n viÃªn thuáº¿", "nhÃ¢n viÃªn háº£i quan",
    "tá»• chá»©c", "tá»• chá»©c phi chÃ­nh phá»§", "tá»• chá»©c quá»‘c táº¿", "tá»• chá»©c tá»« thiá»‡n", "tá»• chá»©c nhÃ¢n Ä‘áº¡o",
    "trÆ°á»ng há»c", "Ä‘áº¡i há»c", "cao Ä‘áº³ng", "trung cáº¥p", "trung há»c", "tiá»ƒu há»c", "máº§m non",
    "ngÆ°á»i quen", "báº¡n bÃ¨", "Ä‘á»“ng nghiá»‡p", "hÃ ng xÃ³m", "ngÆ°á»i thÃ¢n", "gia Ä‘Ã¬nh", "báº¡n há»c",
    "tiktok", "facebook", "zalo", "telegram", "viber", "wechat", "weibo", "line", "kakao talk",
    "Ä‘iá»‡n lá»±c", "báº£o hiá»ƒm", "bÆ°u Ä‘iá»‡n", "cÆ¡ quan thuáº¿", "cÆ¡ quan háº£i quan", "cÆ¡ quan báº£o hiá»ƒm xÃ£ há»™i",
    "cÆ¡ quan báº£o hiá»ƒm y táº¿", "cÆ¡ quan báº£o hiá»ƒm tháº¥t nghiá»‡p", "cÆ¡ quan báº£o hiá»ƒm tai náº¡n lao Ä‘á»™ng"
    
    # === CÃ”NG NGHá»† VÃ€ HACK ===
    "hack", "bá»‹ hack", "tÃ i khoáº£n bá»‹ hack", "báº£o máº­t", "báº£o vá»‡ tÃ i khoáº£n", "khÃ³a tÃ i khoáº£n",
    "tÃ i khoáº£n bá»‹ khÃ³a", "tÃ i khoáº£n bá»‹ Ä‘Ã³ng", "tÃ i khoáº£n bá»‹ Ä‘Ã¬nh chá»‰", "tÃ i khoáº£n bá»‹ táº¡m khÃ³a",
    "deepfake", "video giáº£", "áº£nh giáº£", "tin nháº¯n giáº£", "cuá»™c gá»i giáº£", "cÃ´ng nghá»‡ cao",
    "cÃ´ng nghá»‡ tiÃªn tiáº¿n", "cÃ´ng nghá»‡ má»›i nháº¥t", "cÃ´ng nghá»‡ Ä‘á»™c quyá»n", "cÃ´ng nghá»‡ Ä‘á»™c Ä‘Ã¡o",
    "gá»i Ä‘iá»‡n tá»± Ä‘á»™ng", "gá»i Ä‘iá»‡n robot", "gá»i Ä‘iá»‡n mÃ¡y", "gá»i Ä‘iá»‡n tá»± Ä‘á»™ng", "gá»i Ä‘iá»‡n hÃ ng loáº¡t",
    "email giáº£ máº¡o", "email giáº£", "email lá»«a Ä‘áº£o", "email spam", "email rÃ¡c", "email Ä‘á»™c háº¡i",
    "sim rÃ¡c", "sim sá»‘ Ä‘áº¹p", "sim giáº£", "sim lá»«a Ä‘áº£o", "sim spam", "sim Ä‘á»™c háº¡i",
    "ngÃ¢n hÃ ng giáº£", "ngÃ¢n hÃ ng lá»«a Ä‘áº£o", "ngÃ¢n hÃ ng spam", "ngÃ¢n hÃ ng Ä‘á»™c háº¡i",
    "website giáº£", "website lá»«a Ä‘áº£o", "website spam", "website Ä‘á»™c háº¡i", "app giáº£", "app lá»«a Ä‘áº£o"
    
    # === THÃ”NG TIN CÃ NHÃ‚N ===
    "cmnd", "cccd", "cÄƒn cÆ°á»›c", "há»™ chiáº¿u", "giáº¥y tá» tÃ¹y thÃ¢n", "thÃ´ng tin cÃ¡ nhÃ¢n",
    "thÃ´ng tin riÃªng tÆ°", "thÃ´ng tin bÃ­ máº­t", "thÃ´ng tin nháº¡y cáº£m", "thÃ´ng tin quan trá»ng",
    "sá»‘ Ä‘iá»‡n thoáº¡i", "sá»‘ di Ä‘á»™ng", "sá»‘ mobile", "sá»‘ liÃªn láº¡c", "sá»‘ liÃªn há»‡", "sá»‘ hotline",
    "email", "Ä‘á»‹a chá»‰ email", "email liÃªn há»‡", "email liÃªn láº¡c", "email cÃ´ng viá»‡c", "email cÃ¡ nhÃ¢n",
    "Ä‘á»‹a chá»‰", "Ä‘á»‹a chá»‰ nhÃ ", "Ä‘á»‹a chá»‰ cÆ° trÃº", "Ä‘á»‹a chá»‰ thÆ°á»ng trÃº", "Ä‘á»‹a chá»‰ táº¡m trÃº",
    "ngÃ y sinh", "nÆ¡i sinh", "quÃª quÃ¡n", "quá»‘c tá»‹ch", "dÃ¢n tá»™c", "tÃ´n giÃ¡o", "nghá» nghiá»‡p",
    "thá»«a káº¿", "di chÃºc", "tÃ i sáº£n thá»«a káº¿", "quyá»n thá»«a káº¿", "ngÆ°á»i thá»«a káº¿", "ngÆ°á»i Ä‘Æ°á»£c thá»«a káº¿"
    
    # === DU Lá»ŠCH VÃ€ GIáº¢I TRÃ ===
    "du lá»‹ch", "mÃ¹a du lá»‹ch", "vÃ© concert", "vÃ© mÃ¡y bay", "tour du lá»‹ch", "du lá»‹ch giÃ¡ ráº»",
    "du lá»‹ch khuyáº¿n mÃ£i", "du lá»‹ch giáº£m giÃ¡", "du lá»‹ch sale", "du lá»‹ch Æ°u Ä‘Ã£i", "du lá»‹ch Ä‘áº·c biá»‡t",
    "vÃ© concert", "vÃ© nháº¡c há»™i", "vÃ© show", "vÃ© biá»ƒu diá»…n", "vÃ© ca nháº¡c", "vÃ© hÃ²a nháº¡c",
    "vÃ© mÃ¡y bay", "vÃ© bay", "vÃ© hÃ ng khÃ´ng", "vÃ© khá»© há»“i", "vÃ© má»™t chiá»u", "vÃ© ná»™i Ä‘á»‹a", "vÃ© quá»‘c táº¿",
    "tour du lá»‹ch", "chuyáº¿n du lá»‹ch", "hÃ nh trÃ¬nh du lá»‹ch", "lá»‹ch trÃ¬nh du lá»‹ch", "káº¿ hoáº¡ch du lá»‹ch",
    "Ä‘áº·t phÃ²ng", "Ä‘áº·t khÃ¡ch sáº¡n", "Ä‘áº·t resort", "Ä‘áº·t homestay", "Ä‘áº·t villa", "Ä‘áº·t cÄƒn há»™",
    "vÃ© xem phim", "vÃ© phim", "vÃ© ráº¡p chiáº¿u phim", "vÃ© cinema", "vÃ© movie", "vÃ© show phim",
    "Ä‘áº·t cá»c thuÃª nhÃ ", "Ä‘áº·t cá»c thuÃª phÃ²ng", "Ä‘áº·t cá»c thuÃª cÄƒn há»™", "Ä‘áº·t cá»c thuÃª villa",
    "khÃ¡ch sáº¡n", "resort", "homestay", "villa", "cÄƒn há»™", "nhÃ  nghá»‰", "motel", "hostel"
    
    # === MUA Sáº®M VÃ€ BÃN HÃ€NG ===
    "bÃ¡n hÃ ng online", "mua sáº¯m online", "bÃ¡n hÃ ng trá»±c tuyáº¿n", "bÃ¡n hÃ ng Ä‘a cáº¥p",
    "bÃ¡n hÃ ng qua máº¡ng", "bÃ¡n hÃ ng internet", "bÃ¡n hÃ ng website", "bÃ¡n hÃ ng app",
    "mua hÃ ng khÃ´ng giao", "mua hÃ ng bá»‹ lá»«a", "mua hÃ ng giáº£", "mua hÃ ng kÃ©m cháº¥t lÆ°á»£ng",
    "Ä‘áº·t cá»c", "Ä‘áº·t cá»c mua hÃ ng", "Ä‘áº·t cá»c Ä‘áº·t hÃ ng", "Ä‘áº·t cá»c Ä‘áº·t sáº£n pháº©m",
    "thanh toÃ¡n trÆ°á»›c", "thanh toÃ¡n trÆ°á»›c khi giao hÃ ng", "thanh toÃ¡n trÆ°á»›c khi nháº­n hÃ ng",
    "hoÃ n tiá»n", "hoÃ n vá»‘n", "hoÃ n phÃ­", "hoÃ n phÃ­ váº­n chuyá»ƒn", "hoÃ n phÃ­ giao hÃ ng",
    "xe mÃ¡y", "xe tay ga", "xe sá»‘", "xe cÃ´n tay", "xe mÃ´ tÃ´", "xe gáº¯n mÃ¡y",
    "náº¡p tháº» Ä‘iá»‡n thoáº¡i", "náº¡p tiá»n Ä‘iá»‡n thoáº¡i", "náº¡p tháº» sim", "náº¡p tiá»n sim",
    "sá»‘ Ä‘áº¹p", "sim sá»‘ Ä‘áº¹p", "sá»‘ Ä‘iá»‡n thoáº¡i Ä‘áº¹p", "sá»‘ di Ä‘á»™ng Ä‘áº¹p", "sá»‘ mobile Ä‘áº¹p",
    "sáº£n pháº©m giáº£", "hÃ ng giáº£", "hÃ ng nhÃ¡i", "hÃ ng kÃ©m cháº¥t lÆ°á»£ng", "hÃ ng khÃ´ng chÃ­nh hÃ£ng"
    
    # === VIá»†C LÃ€M VÃ€ TUYá»‚N Dá»¤NG ===
    "tuyá»ƒn dá»¥ng", "viá»‡c nháº¹ lÆ°Æ¡ng cao", "lÆ°Æ¡ng cao", "tuyá»ƒn ngÆ°á»i máº«u", "cÃ´ng viá»‡c online",
    "tuyá»ƒn dá»¥ng online", "tuyá»ƒn dá»¥ng qua máº¡ng", "tuyá»ƒn dá»¥ng internet", "tuyá»ƒn dá»¥ng website",
    "viá»‡c nháº¹ lÆ°Æ¡ng cao", "viá»‡c dá»… lÆ°Æ¡ng cao", "viá»‡c Ä‘Æ¡n giáº£n lÆ°Æ¡ng cao", "viá»‡c khÃ´ng cáº§n kinh nghiá»‡m",
    "lÆ°Æ¡ng cao", "lÆ°Æ¡ng khá»§ng", "lÆ°Æ¡ng tá»‘t", "lÆ°Æ¡ng háº¥p dáº«n", "lÆ°Æ¡ng cáº¡nh tranh", "lÆ°Æ¡ng thá»‹ trÆ°á»ng",
    "tuyá»ƒn ngÆ°á»i máº«u", "tuyá»ƒn diá»…n viÃªn", "tuyá»ƒn ca sÄ©", "tuyá»ƒn ngÆ°á»i Ä‘áº¹p", "tuyá»ƒn ngÆ°á»i ná»•i tiáº¿ng",
    "cÃ´ng viá»‡c online", "viá»‡c lÃ m online", "cÃ´ng viá»‡c qua máº¡ng", "viá»‡c lÃ m qua máº¡ng",
    "part time", "full time", "lÃ m viá»‡c táº¡i nhÃ ", "lÃ m viá»‡c tá»« xa", "lÃ m viá»‡c online",
    "kiáº¿m tiá»n online", "kiáº¿m tiá»n qua máº¡ng", "kiáº¿m tiá»n internet", "kiáº¿m tiá»n website",
    "cÆ¡ há»™i viá»‡c lÃ m", "cÆ¡ há»™i nghá» nghiá»‡p", "cÆ¡ há»™i thÄƒng tiáº¿n", "cÆ¡ há»™i phÃ¡t triá»ƒn"
    
    # === Tá»ª THIá»†N VÃ€ VAY MÆ¯á»¢N ===
    "kÃªu gá»i tá»« thiá»‡n", "quyÃªn gÃ³p", "á»§ng há»™", "vay tiá»n online", "vay tiá»n nhanh",
    "kÃªu gá»i quyÃªn gÃ³p", "kÃªu gá»i á»§ng há»™", "kÃªu gá»i tá»« thiá»‡n", "kÃªu gá»i nhÃ¢n Ä‘áº¡o",
    "quyÃªn gÃ³p", "á»§ng há»™", "há»— trá»£", "giÃºp Ä‘á»¡", "chia sáº»", "Ä‘Ã³ng gÃ³p", "tÃ i trá»£",
    "vay tiá»n online", "vay tiá»n qua máº¡ng", "vay tiá»n internet", "vay tiá»n website",
    "vay tiá»n nhanh", "vay tiá»n gáº¥p", "vay tiá»n kháº©n cáº¥p", "vay tiá»n cáº¥p bÃ¡ch",
    "vay tiá»n khÃ´ng cáº§n tháº¿ cháº¥p", "vay tiá»n khÃ´ng cáº§n báº£o lÃ£nh", "vay tiá»n khÃ´ng cáº§n giáº¥y tá»",
    "vay tiá»n lÃ£i suáº¥t tháº¥p", "vay tiá»n lÃ£i suáº¥t Æ°u Ä‘Ã£i", "vay tiá»n lÃ£i suáº¥t tá»‘t",
    "cáº§m Ä‘á»“", "tháº¿ cháº¥p", "báº£o lÃ£nh", "Ä‘áº£m báº£o", "cam káº¿t", "há»©a háº¹n", "Ä‘áº£m báº£o tráº£ ná»£"
    
    # === GAME VÃ€ GIáº¢I TRÃ ONLINE ===
    "game online", "chÆ¡i game kiáº¿m tiá»n", "game thá»§", "esports", "streaming", "youtube",
    "game online", "game mobile", "game pc", "game console", "game Ä‘iá»‡n tá»­", "game video",
    "chÆ¡i game kiáº¿m tiá»n", "game kiáº¿m tiá»n", "game tháº¯ng tiá»n", "game Ä‘Ã¡nh bÃ i", "game casino",
    "game thá»§", "game player", "ngÆ°á»i chÆ¡i game", "cá»™ng Ä‘á»“ng game", "há»™i game thá»§",
    "esports", "thá»ƒ thao Ä‘iá»‡n tá»­", "thi Ä‘áº¥u game", "giáº£i Ä‘áº¥u game", "champion game",
    "streaming", "phÃ¡t trá»±c tiáº¿p", "live stream", "phÃ¡t sÃ³ng trá»±c tiáº¿p", "phÃ¡t sÃ³ng live",
    "youtube", "tiktok", "instagram", "facebook", "zalo", "telegram", "viber", "wechat", "weibo",
    "máº¡ng xÃ£ há»™i", "social media", "platform", "ná»n táº£ng", "á»©ng dá»¥ng máº¡ng xÃ£ há»™i"
    
    # === QUáº¢NG CÃO VÃ€ MARKETING ===
    "cháº¡y quáº£ng cÃ¡o", "quáº£ng cÃ¡o online", "marketing online", "seo", "google ads", "facebook ads",
    "cháº¡y quáº£ng cÃ¡o", "Ä‘áº·t quáº£ng cÃ¡o", "mua quáº£ng cÃ¡o", "Ä‘Äƒng quáº£ng cÃ¡o", "phÃ¡t quáº£ng cÃ¡o",
    "quáº£ng cÃ¡o online", "quáº£ng cÃ¡o internet", "quáº£ng cÃ¡o máº¡ng", "quáº£ng cÃ¡o website", "quáº£ng cÃ¡o app",
    "marketing online", "marketing internet", "marketing máº¡ng", "marketing sá»‘", "marketing Ä‘iá»‡n tá»­",
    "seo", "tá»‘i Æ°u hÃ³a cÃ´ng cá»¥ tÃ¬m kiáº¿m", "tá»‘i Æ°u seo", "seo website", "seo tá»« khÃ³a",
    "google ads", "facebook ads", "tiktok ads", "youtube ads", "instagram ads", "twitter ads",
    "quáº£ng cÃ¡o google", "quáº£ng cÃ¡o facebook", "quáº£ng cÃ¡o tiktok", "quáº£ng cÃ¡o youtube",
    "influencer", "kols", "ngÆ°á»i ná»•i tiáº¿ng", "ngÆ°á»i cÃ³ áº£nh hÆ°á»Ÿng", "ngÆ°á»i cÃ³ táº§m áº£nh hÆ°á»Ÿng",
    "ngÆ°á»i cÃ³ sá»©c áº£nh hÆ°á»Ÿng", "ngÆ°á»i cÃ³ uy tÃ­n", "ngÆ°á»i cÃ³ danh tiáº¿ng", "ngÆ°á»i cÃ³ tiáº¿ng nÃ³i"
    
    # === Tá»ª NGá»® Cáº¢NH BÃO ===
    "cáº£nh bÃ¡o", "chÃº Ã½", "quan trá»ng", "nghiÃªm trá»ng", "nguy hiá»ƒm", "cáº©n tháº­n",
    "cáº£nh bÃ¡o", "chÃº Ã½", "quan trá»ng", "nghiÃªm trá»ng", "nguy hiá»ƒm", "cáº©n tháº­n",
    "cáº£nh bÃ¡o quan trá»ng", "cáº£nh bÃ¡o kháº©n cáº¥p", "cáº£nh bÃ¡o nguy hiá»ƒm", "cáº£nh bÃ¡o nghiÃªm trá»ng",
    "chÃº Ã½ quan trá»ng", "chÃº Ã½ kháº©n cáº¥p", "chÃº Ã½ nguy hiá»ƒm", "chÃº Ã½ nghiÃªm trá»ng",
    "quan trá»ng", "ráº¥t quan trá»ng", "cá»±c ká»³ quan trá»ng", "vÃ´ cÃ¹ng quan trá»ng",
    "nghiÃªm trá»ng", "ráº¥t nghiÃªm trá»ng", "cá»±c ká»³ nghiÃªm trá»ng", "vÃ´ cÃ¹ng nghiÃªm trá»ng",
    "nguy hiá»ƒm", "ráº¥t nguy hiá»ƒm", "cá»±c ká»³ nguy hiá»ƒm", "vÃ´ cÃ¹ng nguy hiá»ƒm",
    "cáº©n tháº­n", "ráº¥t cáº©n tháº­n", "cá»±c ká»³ cáº©n tháº­n", "vÃ´ cÃ¹ng cáº©n tháº­n",
    "khÃ´ng Ä‘Æ°á»£c chia sáº»", "bÃ­ máº­t", "riÃªng tÆ°", "khÃ´ng Ä‘Æ°á»£c tiáº¿t lá»™", "tuyá»‡t máº­t",
    "khÃ´ng Ä‘Æ°á»£c chia sáº»", "khÃ´ng Ä‘Æ°á»£c tiáº¿t lá»™", "khÃ´ng Ä‘Æ°á»£c nÃ³i", "khÃ´ng Ä‘Æ°á»£c ká»ƒ",
    "bÃ­ máº­t", "riÃªng tÆ°", "tuyá»‡t máº­t", "cá»±c ká»³ bÃ­ máº­t", "vÃ´ cÃ¹ng bÃ­ máº­t"
    
    # === Tá»ª NGá»® THUYáº¾T PHá»¤C ===
    "tin tÃ´i Ä‘i", "Ä‘áº£m báº£o", "cháº¯c cháº¯n", "100%", "khÃ´ng cÃ³ gÃ¬ pháº£i lo",
    "tin tÃ´i Ä‘i", "tin tÃ´i", "hÃ£y tin tÃ´i", "tÃ´i Ä‘áº£m báº£o", "tÃ´i cháº¯c cháº¯n", "tÃ´i cam káº¿t",
    "Ä‘áº£m báº£o", "cháº¯c cháº¯n", "100%", "khÃ´ng cÃ³ gÃ¬ pháº£i lo", "khÃ´ng cÃ³ gÃ¬ pháº£i sá»£",
    "Ä‘áº£m báº£o 100%", "cháº¯c cháº¯n 100%", "khÃ´ng cÃ³ gÃ¬ pháº£i lo", "khÃ´ng cÃ³ gÃ¬ pháº£i sá»£",
    "cÆ¡ há»™i duy nháº¥t", "chá»‰ hÃ´m nay", "giá»›i háº¡n thá»i gian", "sá»‘ lÆ°á»£ng cÃ³ háº¡n",
    "cÆ¡ há»™i duy nháº¥t", "cÆ¡ há»™i cuá»‘i cÃ¹ng", "cÆ¡ há»™i hiáº¿m cÃ³", "cÆ¡ há»™i Ä‘áº·c biá»‡t",
    "chá»‰ hÃ´m nay", "chá»‰ ngÃ y hÃ´m nay", "chá»‰ trong ngÃ y", "chá»‰ trong hÃ´m nay",
    "giá»›i háº¡n thá»i gian", "thá»i gian cÃ³ háº¡n", "thá»i gian giá»›i háº¡n", "thá»i gian cuá»‘i",
    "sá»‘ lÆ°á»£ng cÃ³ háº¡n", "sá»‘ lÆ°á»£ng giá»›i háº¡n", "sá»‘ lÆ°á»£ng cuá»‘i", "sá»‘ lÆ°á»£ng cuá»‘i cÃ¹ng",
    "Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t", "giáº£m giÃ¡ sá»‘c", "khuyáº¿n mÃ£i lá»›n", "sale off",
    "Æ°u Ä‘Ã£i Ä‘áº·c biá»‡t", "Æ°u Ä‘Ã£i cuá»‘i cÃ¹ng", "Æ°u Ä‘Ã£i hiáº¿m cÃ³", "Æ°u Ä‘Ã£i Ä‘á»™c quyá»n",
    "giáº£m giÃ¡ sá»‘c", "giáº£m giÃ¡ khá»§ng", "giáº£m giÃ¡ cá»±c sá»‘c", "giáº£m giÃ¡ khÃ´ng tÆ°á»Ÿng",
    "khuyáº¿n mÃ£i lá»›n", "khuyáº¿n mÃ£i khá»§ng", "khuyáº¿n mÃ£i cá»±c lá»›n", "khuyáº¿n mÃ£i khÃ´ng tÆ°á»Ÿng",
    "sale off", "sale khá»§ng", "sale cá»±c khá»§ng", "sale khÃ´ng tÆ°á»Ÿng"
    
    # === Tá»ª NGá»® Táº O ÃP Lá»°C ===
    "pháº£i lÃ m ngay", "khÃ´ng Ä‘Æ°á»£c cháº­m trá»…", "háº­u quáº£ nghiÃªm trá»ng", "sáº½ bá»‹ pháº¡t",
    "pháº£i lÃ m ngay", "pháº£i lÃ m gáº¥p", "pháº£i lÃ m kháº©n cáº¥p", "pháº£i lÃ m ngay láº­p tá»©c",
    "khÃ´ng Ä‘Æ°á»£c cháº­m trá»…", "khÃ´ng Ä‘Æ°á»£c trÃ¬ hoÃ£n", "khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ lÃ¢u", "khÃ´ng Ä‘Æ°á»£c Ä‘á»ƒ cháº­m",
    "háº­u quáº£ nghiÃªm trá»ng", "háº­u quáº£ khá»§ng khiáº¿p", "háº­u quáº£ khÃ´ng lÆ°á»ng trÆ°á»›c", "háº­u quáº£ Ä‘Ã¡ng sá»£",
    "sáº½ bá»‹ pháº¡t", "sáº½ bá»‹ khÃ³a", "sáº½ bá»‹ máº¥t", "sáº½ bá»‹ Ä‘Ã³ng", "sáº½ bá»‹ há»§y", "sáº½ bá»‹ tá»‹ch thu",
    "sáº½ bá»‹ pháº¡t", "sáº½ bá»‹ pháº¡t tiá»n", "sáº½ bá»‹ pháº¡t náº·ng", "sáº½ bá»‹ pháº¡t ráº¥t náº·ng",
    "sáº½ bá»‹ khÃ³a", "sáº½ bá»‹ Ä‘Ã³ng", "sáº½ bá»‹ Ä‘Ã¬nh chá»‰", "sáº½ bá»‹ táº¡m khÃ³a", "sáº½ bá»‹ táº¡m Ä‘Ã³ng",
    "sáº½ bá»‹ máº¥t", "sáº½ bá»‹ máº¥t vÄ©nh viá»…n", "sáº½ bá»‹ máº¥t hoÃ n toÃ n", "sáº½ bá»‹ máº¥t táº¥t cáº£",
    "sáº½ bá»‹ Ä‘Ã³ng", "sáº½ bá»‹ Ä‘Ã³ng vÄ©nh viá»…n", "sáº½ bá»‹ Ä‘Ã³ng hoÃ n toÃ n", "sáº½ bá»‹ Ä‘Ã³ng táº¥t cáº£",
    "sáº½ bá»‹ há»§y", "sáº½ bá»‹ há»§y vÄ©nh viá»…n", "sáº½ bá»‹ há»§y hoÃ n toÃ n", "sáº½ bá»‹ há»§y táº¥t cáº£",
    "sáº½ bá»‹ tá»‹ch thu", "sáº½ bá»‹ kiá»ƒm tra", "sáº½ bá»‹ Ä‘iá»u tra", "sáº½ bá»‹ báº¯t", "sáº½ bá»‹ pháº¡t tiá»n",
    "sáº½ bá»‹ tá»‹ch thu", "sáº½ bá»‹ tá»‹ch thu hoÃ n toÃ n", "sáº½ bá»‹ tá»‹ch thu táº¥t cáº£", "sáº½ bá»‹ tá»‹ch thu vÄ©nh viá»…n",
    "sáº½ bá»‹ kiá»ƒm tra", "sáº½ bá»‹ Ä‘iá»u tra", "sáº½ bá»‹ báº¯t", "sáº½ bá»‹ pháº¡t tiá»n",
    "sáº½ bá»‹ kiá»ƒm tra", "sáº½ bá»‹ kiá»ƒm tra gáº¯t gao", "sáº½ bá»‹ kiá»ƒm tra nghiÃªm ngáº·t", "sáº½ bá»‹ kiá»ƒm tra ká»¹ lÆ°á»¡ng",
    "sáº½ bá»‹ Ä‘iá»u tra", "sáº½ bá»‹ Ä‘iá»u tra gáº¯t gao", "sáº½ bá»‹ Ä‘iá»u tra nghiÃªm ngáº·t", "sáº½ bá»‹ Ä‘iá»u tra ká»¹ lÆ°á»¡ng",
    "sáº½ bá»‹ báº¯t", "sáº½ bá»‹ báº¯t ngay", "sáº½ bá»‹ báº¯t gáº¥p", "sáº½ bá»‹ báº¯t kháº©n cáº¥p",
    "sáº½ bá»‹ pháº¡t tiá»n", "sáº½ bá»‹ pháº¡t tiá»n náº·ng", "sáº½ bá»‹ pháº¡t tiá»n ráº¥t náº·ng", "sáº½ bá»‹ pháº¡t tiá»n khá»§ng khiáº¿p"
    
    # === Tá»ª NGá»® THá»œI GIAN ===
    "hÃ´m nay", "ngay bÃ¢y giá»", "trong vÃ²ng 24h", "trÆ°á»›c 12h Ä‘Ãªm", "cuá»‘i tuáº§n",
    "hÃ´m nay", "ngÃ y hÃ´m nay", "hÃ´m nay", "ngÃ y hÃ´m nay", "hÃ´m nay", "ngÃ y hÃ´m nay",
    "ngay bÃ¢y giá»", "ngay láº­p tá»©c", "ngay tá»©c kháº¯c", "ngay tá»©c thÃ¬", "ngay tá»©c kháº¯c",
    "trong vÃ²ng 24h", "trong vÃ²ng 1 ngÃ y", "trong vÃ²ng 24 giá»", "trong vÃ²ng 1 ngÃ y",
    "trÆ°á»›c 12h Ä‘Ãªm", "trÆ°á»›c 12 giá» Ä‘Ãªm", "trÆ°á»›c 12 giá» tá»‘i", "trÆ°á»›c 12 giá» tá»‘i",
    "cuá»‘i tuáº§n", "cuá»‘i thÃ¡ng", "cuá»‘i nÄƒm", "táº¿t", "lá»…", "ká»³ nghá»‰", "mÃ¹a cao Ä‘iá»ƒm",
    "cuá»‘i tuáº§n", "cuá»‘i tuáº§n nÃ y", "cuá»‘i tuáº§n tá»›i", "cuá»‘i tuáº§n sau", "cuá»‘i tuáº§n trÆ°á»›c",
    "cuá»‘i thÃ¡ng", "cuá»‘i thÃ¡ng nÃ y", "cuá»‘i thÃ¡ng tá»›i", "cuá»‘i thÃ¡ng sau", "cuá»‘i thÃ¡ng trÆ°á»›c",
    "cuá»‘i nÄƒm", "cuá»‘i nÄƒm nÃ y", "cuá»‘i nÄƒm tá»›i", "cuá»‘i nÄƒm sau", "cuá»‘i nÄƒm trÆ°á»›c",
    "táº¿t", "lá»…", "ká»³ nghá»‰", "mÃ¹a cao Ä‘iá»ƒm", "mÃ¹a du lá»‹ch", "mÃ¹a lá»… há»™i", "mÃ¹a Ä‘áº·c biá»‡t"
    
    # === Tá»ª NGá»® Sá» LÆ¯á»¢NG ===
    "chá»‰ cÃ²n", "sá»‘ lÆ°á»£ng cÃ³ háº¡n", "giá»›i háº¡n", "Ä‘Ã£ háº¿t", "sáº¯p háº¿t", "cuá»‘i cÃ¹ng",
    "chá»‰ cÃ²n", "chá»‰ cÃ²n láº¡i", "chá»‰ cÃ²n sÃ³t láº¡i", "chá»‰ cÃ²n thá»«a láº¡i", "chá»‰ cÃ²n dÆ° láº¡i",
    "sá»‘ lÆ°á»£ng cÃ³ háº¡n", "sá»‘ lÆ°á»£ng giá»›i háº¡n", "sá»‘ lÆ°á»£ng cuá»‘i", "sá»‘ lÆ°á»£ng cuá»‘i cÃ¹ng",
    "giá»›i háº¡n", "giá»›i háº¡n cuá»‘i", "giá»›i háº¡n cuá»‘i cÃ¹ng", "giá»›i háº¡n cuá»‘i cÃ¹ng",
    "Ä‘Ã£ háº¿t", "sáº¯p háº¿t", "cuá»‘i cÃ¹ng", "láº§n cuá»‘i", "cÆ¡ há»™i cuá»‘i", "Æ°u Ä‘Ã£i cuá»‘i", "giáº£m giÃ¡ cuá»‘i",
    "Ä‘Ã£ háº¿t", "Ä‘Ã£ háº¿t sáº¡ch", "Ä‘Ã£ háº¿t hoÃ n toÃ n", "Ä‘Ã£ háº¿t táº¥t cáº£", "Ä‘Ã£ háº¿t vÄ©nh viá»…n",
    "sáº¯p háº¿t", "sáº¯p háº¿t sáº¡ch", "sáº¯p háº¿t hoÃ n toÃ n", "sáº¯p háº¿t táº¥t cáº£", "sáº¯p háº¿t vÄ©nh viá»…n",
    "cuá»‘i cÃ¹ng", "láº§n cuá»‘i", "cÆ¡ há»™i cuá»‘i", "Æ°u Ä‘Ã£i cuá»‘i", "giáº£m giÃ¡ cuá»‘i",
    "cuá»‘i cÃ¹ng", "cuá»‘i cÃ¹ng", "cuá»‘i cÃ¹ng", "cuá»‘i cÃ¹ng", "cuá»‘i cÃ¹ng",
    "láº§n cuá»‘i", "láº§n cuá»‘i cÃ¹ng", "láº§n cuá»‘i cÃ¹ng", "láº§n cuá»‘i cÃ¹ng", "láº§n cuá»‘i cÃ¹ng",
    "cÆ¡ há»™i cuá»‘i", "Æ°u Ä‘Ã£i cuá»‘i", "giáº£m giÃ¡ cuá»‘i", "cÆ¡ há»™i cuá»‘i", "Æ°u Ä‘Ã£i cuá»‘i", "giáº£m giÃ¡ cuá»‘i"
    
    # === Tá»ª NGá»® Äá»ŠA LÃ ===
    "tá»« nÆ°á»›c ngoÃ i", "quá»‘c táº¿", "toÃ n cáº§u", "chÃ¢u Ã¢u", "chÃ¢u má»¹", "chÃ¢u Ã¡",
    "tá»« nÆ°á»›c ngoÃ i", "tá»« quá»‘c táº¿", "tá»« toÃ n cáº§u", "tá»« chÃ¢u Ã¢u", "tá»« chÃ¢u má»¹", "tá»« chÃ¢u Ã¡",
    "quá»‘c táº¿", "toÃ n cáº§u", "chÃ¢u Ã¢u", "chÃ¢u má»¹", "chÃ¢u Ã¡", "chÃ¢u phi", "chÃ¢u Ãºc",
    "singapore", "hong kong", "Ä‘Ã i loan", "hÃ n quá»‘c", "nháº­t báº£n", "trung quá»‘c",
    "singapore", "singapore", "hong kong", "Ä‘Ã i loan", "hÃ n quá»‘c", "nháº­t báº£n", "trung quá»‘c",
    "má»¹", "anh", "Ä‘á»©c", "phÃ¡p", "canada", "australia", "new zealand", "switzerland", "netherlands",
    "má»¹", "anh", "Ä‘á»©c", "phÃ¡p", "canada", "australia", "new zealand", "switzerland", "netherlands",
    "united states", "united kingdom", "germany", "france", "canada", "australia", "new zealand",
    "switzerland", "netherlands", "belgium", "austria", "sweden", "norway", "denmark", "finland"
]

# HÃ m clean_text: LÃ m sáº¡ch há»™i thoáº¡i (lowercase, xÃ³a thá»«a, giá»¯ dáº¥u tiáº¿ng Viá»‡t)
@lru_cache(maxsize=10000)
def clean_text(text: str) -> str:
    if not isinstance(text, str):
        return ""
    text = text.lower()  # Lowercase Ä‘á»ƒ Ä‘á»“ng nháº¥t  
    text = re.sub(r'\s+', ' ', text)  # XÃ³a khoáº£ng tráº¯ng thá»«a
    text = re.sub(r'[^\w\s\.,!?]', '', text)  # Loáº¡i kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n (giá»¯ dáº¥u cÃ¢u)
    return text.strip()

@lru_cache(maxsize=10000)
def cached_tokenize(text):
    return tokenizer(
        text,
        truncation=True,
        padding='max_length', 
        max_length=MAX_LENGTH,
        return_overflowing_tokens=True,
        stride=SLIDING_STRIDE,
    )

# HÃ m tá»‘i Æ°u hÃ³a keywords Ä‘á»ƒ tiáº¿t kiá»‡m RAM
def optimize_keywords():
    """Tá»‘i Æ°u hÃ³a keywords Ä‘á»ƒ tiáº¿t kiá»‡m RAM"""
    global keywords
    # Loáº¡i bá» duplicates vÃ  sáº¯p xáº¿p
    keywords = list(set(keywords))
    keywords.sort()
    print(f"ğŸ” ÄÃ£ tá»‘i Æ°u hÃ³a keywords: {len(keywords)} tá»« khÃ³a duy nháº¥t")
    return keywords

# HÃ m preprocess theo batch: táº¡o cÃ¡c cá»­a sá»• trÆ°á»£t cho má»—i há»™i thoáº¡i trong batch
# Tráº£ vá» danh sÃ¡ch cÃ¡c cá»­a sá»• (pháº³ng) vá»›i orig_id cho phÃ©p gá»™p láº¡i khi Ä‘Ã¡nh giÃ¡
def preprocess_batch(examples, indices):
    out_input_ids = []
    out_attention_mask = []
    out_labels = []
    out_keyword_count = []
    out_core_text = []
    out_extended_text = []
    out_orig_id = []
    out_text = []
    
    texts_ext = examples.get('Hoi_thoai_mo_rong')
    texts_std = examples.get('Há»™i thoáº¡i')
    labels = examples.get('Label')
    
    batch_size = len(indices)
    for i in range(batch_size):
        # Láº¥y cáº£ 2 loáº¡i text
        core_text = texts_std[i] if texts_std is not None else ''
        extended_text = texts_ext[i] if texts_ext is not None else ''
        
        # Clean cáº£ 2 loáº¡i text
        core_conv = clean_text(core_text)
        extended_conv = clean_text(extended_text)
        
        # GhÃ©p 2 text vá»›i [SEP] token Ä‘á»ƒ model phÃ¢n biá»‡t
        full_text = core_conv + " [SEP] " + extended_conv
        
        # Tokenize text Ä‘Ã£ ghÃ©p
        enc = cached_tokenize(full_text)
        num_windows = len(enc['input_ids'])
        
        # TÃ­nh sá»‘ cá»­a sá»• cáº§n thiáº¿t
        num_required = max(3, num_windows // 128)  # Ãt nháº¥t 3 cá»­a sá»•, thÃªm 1 cá»­a sá»• cho má»—i 128 token
        selected_windows = [int(i * (num_windows - 1) / (num_required - 1)) for i in range(num_required)]
        
        # Láº¥y cÃ¡c cá»­a sá»• Ä‘Ã£ chá»n
        for idx in selected_windows:
            # Láº¥y tokens tá»« cá»­a sá»• Ä‘Æ°á»£c chá»n vÃ  chuyá»ƒn vá» list
            window_input_ids = enc['input_ids'][idx]
            window_attention_mask = enc['attention_mask'][idx]
            
            # Chuyá»ƒn tensor vá» list náº¿u cáº§n
            if hasattr(window_input_ids, 'tolist'):
                window_input_ids = window_input_ids.tolist()
            if hasattr(window_attention_mask, 'tolist'):
                window_attention_mask = window_attention_mask.tolist()
            
            # Äáº£m báº£o Ä‘Ã¢y lÃ  list
            if not isinstance(window_input_ids, list):
                window_input_ids = list(window_input_ids)
            if not isinstance(window_attention_mask, list):
                window_attention_mask = list(window_attention_mask)
            
            # Validation: Ä‘áº£m báº£o Ä‘á»™ dÃ i Ä‘Ãºng
            if len(window_input_ids) != MAX_LENGTH:
                print(f"âš ï¸ Warning: Window {idx} cÃ³ Ä‘á»™ dÃ i {len(window_input_ids)} != {MAX_LENGTH}")
            
            # Validation: Ä‘áº£m báº£o attention_mask cÃ³ cÃ¹ng Ä‘á»™ dÃ i
            if len(window_attention_mask) != len(window_input_ids):
                print(f"âš ï¸ Warning: Attention mask vÃ  input_ids cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau: {len(window_attention_mask)} vs {len(window_input_ids)}")
            
            out_input_ids.append(window_input_ids)
            out_attention_mask.append(window_attention_mask)
            out_labels.append(int(labels[i]))
            kw_count = sum(1 for kw in keywords if kw in (core_conv + extended_conv))
            out_keyword_count.append(kw_count)
            # LÆ°u text Ä‘Ã£ ghÃ©p
            out_text.append(full_text)
            # LÆ°u core_text vÃ  extended_text cho má»—i cá»­a sá»•
            out_core_text.append(core_conv)
            out_extended_text.append(extended_conv)
            out_orig_id.append(int(indices[i]))
     
    # Debug: kiá»ƒm tra format trÆ°á»›c khi return
    if out_input_ids and out_attention_mask:
        sample_input_ids = out_input_ids[0]
        sample_attention_mask = out_attention_mask[0]
        print(f"ğŸ” Debug - Sample input_ids type: {type(sample_input_ids)}, length: {len(sample_input_ids) if isinstance(sample_input_ids, list) else 'N/A'}")
        print(f"ğŸ” Debug - Sample attention_mask type: {type(sample_attention_mask)}, length: {len(sample_attention_mask) if isinstance(sample_attention_mask, list) else 'N/A'}")
        print(f"ğŸ” Debug - First 5 input_ids: {sample_input_ids[:5] if isinstance(sample_input_ids, list) else sample_input_ids}")
        print(f"ğŸ” Debug - First 5 attention_mask: {sample_attention_mask[:5] if isinstance(sample_attention_mask, list) else sample_attention_mask}")
    
    return {
        'input_ids': out_input_ids,
        'attention_mask': out_attention_mask,
        'labels': out_labels,
        'keyword_count': out_keyword_count,
        'core_text': out_core_text,
        'extended_text': out_extended_text,
        'orig_id': out_orig_id,
        'text': out_text,
    }

# HÃ m tiáº¿t kiá»‡m RAM: xá»­ lÃ½ tá»«ng chunk nhá»
def preprocess_chunk(chunk_data, chunk_id):
    """Xá»­ lÃ½ tá»«ng chunk nhá» Ä‘á»ƒ tiáº¿t kiá»‡m RAM"""
    print(f"ğŸ”„ Äang xá»­ lÃ½ chunk {chunk_id + 1}...")
    
    # Chuyá»ƒn chunk thÃ nh Dataset
    chunk_dataset = Dataset.from_pandas(chunk_data)
    
    # Preprocess chunk vá»›i batch size nhá»
    processed_chunk = chunk_dataset.map(
        preprocess_batch,
        with_indices=True,
        batched=True,
        batch_size=BATCH_SIZE,
        num_proc=1,
        remove_columns=chunk_dataset.column_names,
    )
    
    # Giáº£i phÃ³ng bá»™ nhá»› chunk gá»‘c
    del chunk_dataset
    gc.collect()
    
    return processed_chunk

# HÃ m theo dÃµi RAM
def monitor_memory():
    """Theo dÃµi sá»­ dá»¥ng RAM"""
    try:
        process = psutil.Process()
        memory_info = process.memory_info()
        memory_mb = memory_info.rss / 1024 / 1024
        print(f"ğŸ’¾ RAM hiá»‡n táº¡i: {memory_mb:.1f} MB")
        return memory_mb
    except:
        print("âš ï¸ KhÃ´ng thá»ƒ theo dÃµi RAM (psutil khÃ´ng cÃ³ sáºµn)")
        return 0

# Main function: Äá»c file, preprocess, stratified split (handle imbalance), lÆ°u dataset
def preprocess_dataset(input_file: str, output_dir: str = './preprocessed_scam_dataset'):
    """
    input_file: ÄÆ°á»ng dáº«n Ä‘áº¿n augmented_expanded_scam_dataset.csv
    output_dir: ThÆ° má»¥c lÆ°u DatasetDict tokenized (dá»… load cho train)
    """
    global CHUNK_SIZE  # Khai bÃ¡o global á»Ÿ Ä‘áº§u function
    
    print("ğŸš€ Báº¯t Ä‘áº§u preprocessing vá»›i chiáº¿n lÆ°á»£c tiáº¿t kiá»‡m RAM...")
    monitor_memory()
    
    # Äá»c CSV theo chunk Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    print(f"ğŸ“– Äang Ä‘á»c file: {input_file}")
    chunk_list = []
    total_rows = 0
    
    # Äá»c file theo chunk
    for chunk_id, chunk_df in enumerate(pd.read_csv(input_file, encoding='utf-8', chunksize=CHUNK_SIZE)):
        chunk_list.append(chunk_df)
        total_rows += len(chunk_df)
        print(f"ğŸ“Š ÄÃ£ Ä‘á»c chunk {chunk_id + 1}: {len(chunk_df)} máº«u")
        monitor_memory()
    
    print(f"âœ… Tá»•ng sá»‘ máº«u: {total_rows}")
    
    # Xá»­ lÃ½ tá»«ng chunk Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    processed_chunks = []
    for chunk_id, chunk_df in enumerate(chunk_list):
        print(f"\nğŸ”„ Xá»­ lÃ½ chunk {chunk_id + 1}/{len(chunk_list)}")
        
        # Xá»­ lÃ½ chunk
        processed_chunk = preprocess_chunk(chunk_df, chunk_id)
        processed_chunks.append(processed_chunk)
        
        # Giáº£i phÃ³ng bá»™ nhá»› chunk gá»‘c
        del chunk_df
        gc.collect()
        monitor_memory()
        
        # Náº¿u RAM quÃ¡ cao, force garbage collection
        current_ram = monitor_memory()
        if current_ram > MAX_MEMORY_MB:  # Náº¿u RAM vÆ°á»£t quÃ¡ giá»›i háº¡n
            print(f"âš ï¸ RAM cao ({current_ram:.1f} MB), Ä‘ang force garbage collection...")
            gc.collect()
            import time
            time.sleep(3)  # Äá»£i lÃ¢u hÆ¡n Ä‘á»ƒ há»‡ thá»‘ng giáº£i phÃ³ng bá»™ nhá»›
            
            # Náº¿u váº«n cao, giáº£m chunk size
            if monitor_memory() > MAX_MEMORY_MB:
                print("ğŸš¨ RAM váº«n cao, Ä‘ang giáº£m chunk size...")
                CHUNK_SIZE = max(50, CHUNK_SIZE // 2)  # Giáº£m xuá»‘ng tá»‘i thiá»ƒu 50
                print(f"ğŸ“‰ Chunk size má»›i: {CHUNK_SIZE}")
                
                # Force garbage collection thÃªm
                gc.collect()
                time.sleep(2)
    
    # Giáº£i phÃ³ng bá»™ nhá»› chunk list
    del chunk_list
    gc.collect()
    
    print("\nğŸ”— Äang gá»™p cÃ¡c chunk Ä‘Ã£ xá»­ lÃ½...")
    
    # Gá»™p cÃ¡c chunk Ä‘Ã£ xá»­ lÃ½ theo batch nhá» Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    print("ğŸ”— Äang gá»™p cÃ¡c chunk theo batch nhá»...")
    
    # Import concatenate_datasets
    from datasets import concatenate_datasets
    
    # Gá»™p tá»«ng batch nhá» Ä‘á»ƒ trÃ¡nh OOM
    batch_size = 3  # Gá»™p 3 chunk má»™t láº§n
    preprocessed_dataset = None
    
    for i in range(0, len(processed_chunks), batch_size):
        batch_chunks = processed_chunks[i:i+batch_size]
        print(f"ğŸ”„ Gá»™p batch {i//batch_size + 1}: chunks {i+1}-{min(i+batch_size, len(processed_chunks))}")
        
        if preprocessed_dataset is None:
            preprocessed_dataset = batch_chunks[0]
            for chunk in batch_chunks[1:]:
                preprocessed_dataset = concatenate_datasets([preprocessed_dataset, chunk])
        else:
            for chunk in batch_chunks:
                preprocessed_dataset = concatenate_datasets([preprocessed_dataset, chunk])
        
        # Giáº£i phÃ³ng bá»™ nhá»› batch Ä‘Ã£ xá»­ lÃ½
        del batch_chunks
        gc.collect()
        monitor_memory()
    
    # Giáº£i phÃ³ng bá»™ nhá»› cÃ¡c chunk Ä‘Ã£ xá»­ lÃ½
    del processed_chunks
    gc.collect()
    monitor_memory()
    
    print("âœ… ÄÃ£ gá»™p xong cÃ¡c chunk!")

    # Giá»¯ cÃ¡c cá»™t cáº§n thiáº¿t
    print("ğŸ§¹ Äang dá»n dáº¹p cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t...")
    keep_cols = ['input_ids', 'attention_mask', 'labels', 'keyword_count', 'text', 'orig_id']
    cols_to_drop = [c for c in preprocessed_dataset.column_names if c not in keep_cols]
    if cols_to_drop:
        try:
            preprocessed_dataset = preprocessed_dataset.remove_columns(cols_to_drop)
            print(f"âœ… ÄÃ£ loáº¡i bá» {len(cols_to_drop)} cá»™t khÃ´ng cáº§n thiáº¿t")
        except Exception as e:
            print(f"âš ï¸ KhÃ´ng thá»ƒ loáº¡i bá» cá»™t: {e}")
            print("ğŸ”„ Tiáº¿p tá»¥c vá»›i cá»™t hiá»‡n cÃ³...")
    
    # Chuyá»ƒn cá»™t labels sang ClassLabel theo batch nhá» Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    print("ğŸ·ï¸ Äang chuyá»ƒn Ä‘á»•i labels sang ClassLabel...")
    
    # Chia dataset thÃ nh cÃ¡c batch nhá» Ä‘á»ƒ xá»­ lÃ½
    total_size = len(preprocessed_dataset)
    batch_size_labels = 100  # Giáº£m xuá»‘ng 100 máº«u má»™t láº§n Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    
    processed_batches = []
    for i in range(0, total_size, batch_size_labels):
        end_idx = min(i + batch_size_labels, total_size)
        print(f"ğŸ”„ Xá»­ lÃ½ labels batch {i//batch_size_labels + 1}: {i+1}-{end_idx}")
        
        try:
            # Láº¥y batch nhá»
            batch_dataset = preprocessed_dataset.select(range(i, end_idx))
            
            # Chuyá»ƒn Ä‘á»•i labels cho batch nÃ y
            batch_dataset = batch_dataset.cast_column('labels', ClassLabel(names=['0', '1']))
            
            processed_batches.append(batch_dataset)
            
            # Giáº£i phÃ³ng bá»™ nhá»› batch
            del batch_dataset
            gc.collect()
            monitor_memory()
            
        except Exception as e:
            print(f"âš ï¸ Lá»—i khi xá»­ lÃ½ batch {i//batch_size_labels + 1}: {e}")
            print("ğŸ”„ Thá»­ giáº£m batch size...")
            
            # Thá»­ vá»›i batch size nhá» hÆ¡n
            try:
                smaller_batch_size = batch_size_labels // 2
                for j in range(i, end_idx, smaller_batch_size):
                    sub_end = min(j + smaller_batch_size, end_idx)
                    sub_batch = preprocessed_dataset.select(range(j, sub_end))
                    sub_batch = sub_batch.cast_column('labels', ClassLabel(names=['0', '1']))
                    processed_batches.append(sub_batch)
                    del sub_batch
                    gc.collect()
            except Exception as e2:
                print(f"âŒ KhÃ´ng thá»ƒ xá»­ lÃ½ batch nÃ y: {e2}")
                continue
    
    # Gá»™p láº¡i cÃ¡c batch Ä‘Ã£ xá»­ lÃ½
    if processed_batches:
        preprocessed_dataset = concatenate_datasets(processed_batches)
        del processed_batches
        gc.collect()
        monitor_memory()
    else:
        print("âŒ KhÃ´ng thá»ƒ xá»­ lÃ½ labels, sá»­ dá»¥ng labels gá»‘c")
        # Giá»¯ nguyÃªn labels gá»‘c náº¿u khÃ´ng thá»ƒ chuyá»ƒn Ä‘á»•i
    
    print("âœ‚ï¸ Äang chia dataset thÃ nh train/val/test...")
    
    # Split trá»±c tiáº¿p trÃªn HF Dataset theo batch nhá» Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    print("âœ‚ï¸ Äang chia dataset thÃ nh train/val/test theo batch...")
    
    # TÃ­nh toÃ¡n kÃ­ch thÆ°á»›c split
    total_size = len(preprocessed_dataset)
    train_size = int(0.8 * total_size)
    val_size = int(0.1 * total_size)
    test_size = total_size - train_size - val_size
    
    print(f"ğŸ“Š KÃ­ch thÆ°á»›c: Train={train_size}, Val={val_size}, Test={test_size}")
    
    # Split theo index Ä‘á»ƒ tiáº¿t kiá»‡m RAM
    train_indices = list(range(train_size))
    val_indices = list(range(train_size, train_size + val_size))
    test_indices = list(range(train_size + val_size, total_size))
    
    # Táº¡o cÃ¡c dataset con
    train_ds = preprocessed_dataset.select(train_indices)
    val_ds = preprocessed_dataset.select(val_indices)
    test_ds = preprocessed_dataset.select(test_indices)
    
    # Giáº£i phÃ³ng bá»™ nhá»› dataset gá»‘c
    del preprocessed_dataset
    gc.collect()
    monitor_memory()

    # Táº¡o DatasetDict
    try:
        final_dataset = DatasetDict({
            'train': train_ds,
            'validation': val_ds,
            'test': test_ds
        })
        
        # LÆ°u dataset tokenized theo tá»«ng pháº§n Ä‘á»ƒ tiáº¿t kiá»‡m RAM
        print(f"ğŸ’¾ Äang lÆ°u dataset vÃ o: {output_dir}")
    except Exception as e:
        print(f"âŒ Lá»—i khi táº¡o DatasetDict: {e}")
        return None  # Tráº£ vá» None náº¿u cÃ³ lá»—i
    
    try:
        # LÆ°u tá»«ng pháº§n riÃªng biá»‡t
        os.makedirs(output_dir, exist_ok=True)
        
        print("ğŸ’¾ LÆ°u train dataset...")
        train_ds.save_to_disk(os.path.join(output_dir, 'train'))
        
        print("ğŸ’¾ LÆ°u validation dataset...")
        val_ds.save_to_disk(os.path.join(output_dir, 'validation'))
        
        print("ğŸ’¾ LÆ°u test dataset...")
        test_ds.save_to_disk(os.path.join(output_dir, 'test'))
        
        print(f"âœ… Dataset preprocessed Ä‘Ã£ lÆ°u thÃ nh cÃ´ng táº¡i: {output_dir}")
        print(f"ğŸ“Š KÃ­ch thÆ°á»›c (cá»­a sá»•): Train={len(final_dataset['train'])}, Val={len(final_dataset['validation'])}, Test={len(final_dataset['test'])}")
        
    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u dataset: {e}")
        # Thá»­ lÆ°u vÃ o thÆ° má»¥c local náº¿u khÃ´ng lÆ°u Ä‘Æ°á»£c
        local_output = './preprocessed_scam_dataset'
        print(f"Thá»­ lÆ°u vÃ o thÆ° má»¥c local: {local_output}")
        
        try:
            os.makedirs(local_output, exist_ok=True)
            train_ds.save_to_disk(os.path.join(local_output, 'train'))
            val_ds.save_to_disk(os.path.join(local_output, 'validation'))
            test_ds.save_to_disk(os.path.join(local_output, 'test'))
            print(f"âœ… ÄÃ£ lÆ°u vÃ o thÆ° má»¥c local: {local_output}")
        except Exception as e2:
            print(f"âŒ KhÃ´ng thá»ƒ lÆ°u vÃ o local: {e2}")
            print("ğŸ’¡ Gá»£i Ã½: Giáº£m CHUNK_SIZE hoáº·c BATCH_SIZE Ä‘á»ƒ tiáº¿t kiá»‡m RAM")
    
    try:
        # Giáº£i phÃ³ng bá»™ nhá»› cuá»‘i cÃ¹ng
        del train_ds, val_ds, test_ds
        gc.collect()
        monitor_memory()
        
        return final_dataset
    except Exception as e:
        print(f"âŒ Lá»—i khi giáº£i phÃ³ng bá»™ nhá»›: {e}")
        return None  # Tráº£ vá» None náº¿u cÃ³ lá»—i

# HÃ m load dataset Ä‘Ã£ lÆ°u
def load_preprocessed_dataset(dataset_dir):
    """Load dataset Ä‘Ã£ Ä‘Æ°á»£c preprocess vÃ  lÆ°u"""
    try:
        from datasets import load_from_disk
        dataset = DatasetDict({
            'train': load_from_disk(os.path.join(dataset_dir, 'train')),
            'validation': load_from_disk(os.path.join(dataset_dir, 'validation')),
            'test': load_from_disk(os.path.join(dataset_dir, 'test'))
        })
        print(f"âœ… ÄÃ£ load dataset tá»«: {dataset_dir}")
        return dataset
    except Exception as e:
        print(f"âŒ Lá»—i khi load dataset: {e}")
        return None

# Cháº¡y main function chá»‰ khi file Ä‘Æ°á»£c cháº¡y trá»±c tiáº¿p
if __name__ == '__main__':
    # Tá»‘i Æ°u hÃ³a keywords trÆ°á»›c khi cháº¡y
    optimize_keywords()
    
    input_file = 'augmented_expanded_scam_dataset.csv'
    output_dir = './preprocessed_scam_dataset'
    
    try:
        preprocessed_dataset = preprocess_dataset(input_file, output_dir)
        
        if preprocessed_dataset is not None:
            # Hiá»ƒn thá»‹ sample
            sample = preprocessed_dataset['train'][0]
            print("\nğŸ“‹ Sample sau preprocess:")
            print("Input IDs:", sample['input_ids'])
            print("Attention Mask:", sample['attention_mask'])
            print("Label:", sample['labels'])
            print("Keyword Count:", sample['keyword_count'])
            print("Orig ID:", sample['orig_id'])
            
            # In thÃªm thÃ´ng tin vá» kÃ­ch thÆ°á»›c
            print("\nğŸ“Š ThÃ´ng tin dataset:")
            for split in ['train', 'validation', 'test']:
                print(f"Sá»‘ lÆ°á»£ng máº«u {split}: {len(preprocessed_dataset[split])}")
                
            # In thÃ´ng tin vá» cáº¥u trÃºc dá»¯ liá»‡u
            print("\nğŸ” Cáº¥u trÃºc dá»¯ liá»‡u:")
            for key, value in sample.items():
                print(f"{key}: {type(value)}")
            
            # In thÃªm thÃ´ng tin vá» kÃ­ch thÆ°á»›c
            print("\nğŸ“Š ThÃ´ng tin dataset:")
            print(f"Sá»‘ lÆ°á»£ng máº«u train: {len(preprocessed_dataset['train'])}")
            print(f"Sá»‘ lÆ°á»£ng máº«u validation: {len(preprocessed_dataset['validation'])}")
            print(f"Sá»‘ lÆ°á»£ng máº«u test: {len(preprocessed_dataset['test'])}")
            
            # In thÃªm thÃ´ng tin vá» kÃ­ch thÆ°á»›c
            print("\nğŸ“Š ThÃ´ng tin dataset:")
            print(f"Sá»‘ lÆ°á»£ng máº«u train: {len(preprocessed_dataset['train'])}")
            print(f"Sá»‘ lÆ°á»£ng máº«u validation: {len(preprocessed_dataset['validation'])}")
            print(f"Sá»‘ lÆ°á»£ng máº«u test: {len(preprocessed_dataset['test'])}")
            
            # In thÃ´ng tin vá» cáº¥u trÃºc dá»¯ liá»‡u
            print("\nğŸ” Cáº¥u trÃºc dá»¯ liá»‡u vÃ  ná»™i dung:")
            for key, value in sample.items():
                print(f"\n{key}:")
                print(f"Kiá»ƒu dá»¯ liá»‡u: {type(value)}")
                if key == 'text':
                    print("Ná»™i dung text Ä‘áº§y Ä‘á»§:", value)
                    print("Äá»™ dÃ i text:", len(value))
                    # Hiá»ƒn thá»‹ má»™t sá»‘ tá»« khÃ³a tÃ¬m tháº¥y trong text
                    found_keywords = [kw for kw in keywords if kw in value]
                    if found_keywords:
                        print("CÃ¡c tá»« khÃ³a tÃ¬m tháº¥y:", found_keywords)
        else:
            print("âŒ KhÃ´ng thá»ƒ táº¡o dataset")
        
        # Giáº£i phÃ³ng bá»™ nhá»›
        del preprocessed_dataset
        gc.collect()
        
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh preprocessing: {e}")
        import traceback
        traceback.print_exc()