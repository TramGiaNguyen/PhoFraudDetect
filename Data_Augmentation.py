# Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt
import pandas as pd
import random
from googletrans import Translator  # C√†i ƒë·∫∑t: pip install googletrans==4.0.0-rc1 (n·∫øu ch∆∞a c√≥)
import re  # ƒê·ªÉ l√†m s·∫°ch text n·∫øu c·∫ßn
import asyncio
import json
import os
from datetime import datetime

# Kh·ªüi t·∫°o Translator cho back-translation
translator = Translator()

# ƒê·ªãnh nghƒ©a dict synonym cho replacement (rule-based, d·ª±a tr√™n c√°c t·ª´ ph·ªï bi·∫øn trong dataset l·ª´a ƒë·∫£o)
# B·∫°n c√≥ th·ªÉ m·ªü r·ªông dict n√†y d·ª±a tr√™n dataset
synonyms = {
    "chuy·ªÉn": ["g·ª≠i", "chuy·ªÉn kho·∫£n", "n·ªôp"],
    "ngay": ["l·∫≠p t·ª©c", "g·∫•p", "nhanh ch√≥ng"],
    "OTP": ["m√£ x√°c th·ª±c", "m√£ OTP", "code x√°c nh·∫≠n"],
    "t√†i kho·∫£n": ["STK", "s·ªë t√†i kho·∫£n", "account"],
    "qu√©t": ["scan", "qu√©t m√£"],
    "QR": ["m√£ QR", "QR code"],
    "tr√∫ng th∆∞·ªüng": ["tr√∫ng gi·∫£i", "nh·∫≠n th∆∞·ªüng", "gi·∫£i th∆∞·ªüng"],
    "ph√≠": ["l·ªá ph√≠", "chi ph√≠", "ti·ªÅn ph√≠"],
    "l·ª´a": ["gian l·∫≠n", "l·ª´a ƒë·∫£o"],  # Th√™m n·∫øu c·∫ßn, nh∆∞ng tr√°nh thay ƒë·ªïi nghƒ©a
    # Th√™m nhi·ªÅu h∆°n d·ª±a tr√™n dataset c·ªßa b·∫°n
}

# Danh s√°ch noise sentences ƒë·ªÉ th√™m v√†o h·ªôi tho·∫°i (m√¥ ph·ªèng h·ªôi tho·∫°i d√†i h∆°n)
noises = [
    "Ch√†o anh/ch·ªã, ",
    "B·∫°n kh·ªèe kh√¥ng? ",
    "T√¥i g·ªçi t·ª´ c√¥ng ty... ",
    "Xin l·ªói l√†m phi·ªÅn, nh∆∞ng... ",
    "B·∫°n c√≥ th·ªùi gian kh√¥ng? ",
    "ƒê√¢y l√† cu·ªôc g·ªçi quan tr·ªçng... ",
    "T√¥i mu·ªën x√°c nh·∫≠n th√¥ng tin... ",
]

# H√†m back_translate: D·ªãch sang ti·∫øng Anh r·ªìi ng∆∞·ª£c v·ªÅ ti·∫øng Vi·ªát ƒë·ªÉ t·∫°o bi·∫øn th·ªÉ
async def back_translate(text: str) -> str:
    try:
        # D·ªãch sang ti·∫øng Anh
        en_text = await translator.translate(text, dest='en')
        # D·ªãch ng∆∞·ª£c v·ªÅ ti·∫øng Vi·ªát
        vi_text = await translator.translate(en_text.text, dest='vi')
        return vi_text.text
    except Exception as e:
        print(f"L·ªói back-translation: {e}")
        return text  # Gi·ªØ nguy√™n n·∫øu l·ªói

# H√†m back_translate v·ªõi retry v√† fallback
async def back_translate_with_fallback(text: str, max_retries: int = 3) -> str:
    for attempt in range(max_retries):
        try:
            # D·ªãch sang ti·∫øng Anh
            en_text = await translator.translate(text, dest='en')
            if not en_text or not en_text.text:
                continue
                
            # D·ªãch ng∆∞·ª£c v·ªÅ ti·∫øng Vi·ªát
            vi_text = await translator.translate(en_text.text, dest='vi')
            if vi_text and vi_text.text:
                return vi_text.text
                
        except Exception as e:
            if attempt == max_retries - 1:  # L·∫ßn th·ª≠ cu·ªëi
                print(f"Back-translation th·∫•t b·∫°i sau {max_retries} l·∫ßn th·ª≠: {e}")
                raise e  # N√©m l·ªói ƒë·ªÉ x·ª≠ l√Ω ·ªü c·∫•p cao h∆°n
            else:
                print(f"L·∫ßn th·ª≠ {attempt + 1} th·∫•t b·∫°i, th·ª≠ l·∫°i sau 2 gi√¢y...")
                await asyncio.sleep(2)  # ƒê·ª£i 2 gi√¢y tr∆∞·ªõc khi th·ª≠ l·∫°i
    
    return text  # Gi·ªØ nguy√™n n·∫øu t·∫•t c·∫£ ƒë·ªÅu th·∫•t b·∫°i

# H√†m add_noise: Th√™m c√¢u noise ng·∫´u nhi√™n v√†o ƒë·∫ßu ho·∫∑c cu·ªëi h·ªôi tho·∫°i ƒë·ªÉ m√¥ ph·ªèng d√†i h∆°n
def add_noise(text: str) -> str:
    if random.random() < 0.5:  # X√°c su·∫•t 50% th√™m noise
        noise = random.choice(noises)
        if random.random() < 0.5:
            text = noise + text  # Th√™m ƒë·∫ßu
        else:
            text = text + " " + noise  # Th√™m cu·ªëi
    return text

# H√†m synonym_replacement: Thay th·∫ø t·ª´ ƒë·ªìng nghƒ©a v·ªõi x√°c su·∫•t
def synonym_replacement(text: str) -> str:
    words = text.split()  # T√°ch t·ª´ (ƒë∆°n gi·∫£n, kh√¥ng d√πng tokenizer ph·ª©c t·∫°p)
    for i, word in enumerate(words):
        if word in synonyms and random.random() < 0.3:  # X√°c su·∫•t 30% thay th·∫ø
            replacement = random.choice(synonyms[word])
            words[i] = replacement
    return ' '.join(words)

# H√†m clean_text: L√†m s·∫°ch text c∆° b·∫£n (lowercase, x√≥a kho·∫£ng tr·∫Øng th·ª´a, gi·ªØ d·∫•u ti·∫øng Vi·ªát)
def clean_text(text: str) -> str:
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # X√≥a kho·∫£ng tr·∫Øng th·ª´a
    text = re.sub(r'[^\w\s\.,!?]', '', text)  # Lo·∫°i k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn (gi·ªØ d·∫•u c√¢u)
    return text.strip()

# H√†m augment_example: √Åp d·ª•ng augmentation cho m·ªôt h√†ng d·ªØ li·ªáu
async def augment_example(row, variant_counter, use_extended=True):
    # Ch·ªçn ngu·ªìn text ƒë·ªÉ augment
    if use_extended and pd.notna(row['Hoi_thoai_mo_rong']) and row['Hoi_thoai_mo_rong'].strip():
        # S·ª≠ d·ª•ng Hoi_thoai_mo_rong n·∫øu c√≥ v√† kh√¥ng r·ªóng
        original_conversation = row['Hoi_thoai_mo_rong']
        source_column = 'Hoi_thoai_mo_rong'
    else:
        # Fallback v·ªÅ H·ªôi tho·∫°i n·∫øu Hoi_thoai_mo_rong r·ªóng
        original_conversation = row['H·ªôi tho·∫°i']
        source_column = 'H·ªôi tho·∫°i'
    
    # Clean text
    conversation = clean_text(original_conversation)
    
    # Synonym replacement
    conversation = synonym_replacement(conversation)
    
    # Back-translation v·ªõi x√°c su·∫•t 40%
    if random.random() < 0.4:
        conversation = await back_translate_with_fallback(conversation)
    
    # Add noise
    conversation = add_noise(conversation)
    
    # T·∫°o row m·ªõi
    augmented_row = row.copy()
    augmented_row['H·ªôi tho·∫°i'] = conversation  # Lu√¥n l∆∞u v√†o c·ªôt H·ªôi tho·∫°i
    augmented_row['Hoi_thoai_mo_rong'] = conversation  # C≈©ng l∆∞u v√†o c·ªôt m·ªü r·ªông
    augmented_row['Variant'] = f"aug_{variant_counter}_{source_column[:3]}"  # ƒê√°nh d·∫•u ngu·ªìn g·ªëc
    augmented_row['Original_Index'] = row.name  # Gi·ªØ index g·ªëc ƒë·ªÉ trace back
    return augmented_row

# H√†m l∆∞u checkpoint
def save_checkpoint(checkpoint_file: str, current_index: int, total_processed: int, augmented_rows: list):
    checkpoint_data = {
        'current_index': current_index,
        'total_processed': total_processed,
        'timestamp': datetime.now().isoformat(),
        'status': 'in_progress'
    }
    
    with open(checkpoint_file, 'w', encoding='utf-8') as f:
        json.dump(checkpoint_data, f, ensure_ascii=False, indent=2)
    
    print(f"üíæ Checkpoint ƒë√£ l∆∞u: index {current_index}, ƒë√£ x·ª≠ l√Ω {total_processed} m·∫´u")

# H√†m load checkpoint
def load_checkpoint(checkpoint_file: str):
    if os.path.exists(checkpoint_file):
        try:
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                checkpoint_data = json.load(f)
            
            print(f"üìÇ T√¨m th·∫•y checkpoint: index {checkpoint_data['current_index']}, ƒë√£ x·ª≠ l√Ω {checkpoint_data['total_processed']} m·∫´u")
            return checkpoint_data
        except Exception as e:
            print(f"‚ö†Ô∏è  Kh√¥ng th·ªÉ ƒë·ªçc checkpoint: {e}")
    
    return None

# H√†m l∆∞u file t·∫°m th·ªùi
def save_temp_file(temp_file: str, augmented_rows: list):
    temp_df = pd.DataFrame(augmented_rows)
    temp_df.to_csv(temp_file, index=False, encoding='utf-8-sig')
    print(f"üíæ File t·∫°m th·ªùi ƒë√£ l∆∞u: {temp_file}")

# Main function: ƒê·ªçc file, augment, l∆∞u file m·ªõi v·ªõi checkpoint
async def augment_dataset_with_checkpoint(input_file: str, output_file: str, augmentation_factor: float = 1.0, 
                                       batch_size: int = 100, checkpoint_file: str = 'augmentation_checkpoint.json'):
    """
    augmentation_factor: S·ªë l·∫ßn augment m·ªói m·∫´u
    batch_size: S·ªë m·∫´u x·ª≠ l√Ω tr∆∞·ªõc khi l∆∞u checkpoint
    checkpoint_file: File l∆∞u tr·∫°ng th√°i ƒë·ªÉ resume
    """
    # ƒê·ªçc CSV
    df = pd.read_csv(input_file, encoding='utf-8')
    
    # Ki·ªÉm tra c·∫•u tr√∫c file
    expected_columns = ['Lo·∫°i l·ª´a ƒë·∫£o', 'H·ªôi tho·∫°i', 'Label', 'Hoi_thoai_mo_rong', 'Variant', 'Original_Index']
    missing_columns = [col for col in expected_columns if col not in df.columns]
    
    if missing_columns:
        print(f"Warning: Thi·∫øu c√°c c·ªôt sau: {missing_columns}")
        print(f"C√°c c·ªôt hi·ªán c√≥: {list(df.columns)}")
    
    print(f"üìä S·ªë l∆∞·ª£ng m·∫´u g·ªëc: {len(df)}")
    
    # Th·ªëng k√™ v·ªÅ vi·ªác s·ª≠ d·ª•ng c√°c c·ªôt
    extended_count = df['Hoi_thoai_mo_rong'].notna().sum()
    basic_count = len(df) - extended_count
    print(f"üìä S·ªë m·∫´u c√≥ Hoi_thoai_mo_rong: {extended_count}")
    print(f"üìä S·ªë m·∫´u ch·ªâ c√≥ H·ªôi tho·∫°i: {basic_count}")
    
    # Ki·ªÉm tra checkpoint
    checkpoint = load_checkpoint(checkpoint_file)
    start_index = 0
    augmented_rows = []
    
    if checkpoint:
        start_index = checkpoint['current_index']
        print(f"üîÑ Ti·∫øp t·ª•c t·ª´ index {start_index}")
        
        # Load file t·∫°m th·ªùi n·∫øu c√≥
        temp_file = f"temp_augmented_{start_index}.csv"
        if os.path.exists(temp_file):
            temp_df = pd.read_csv(temp_file, encoding='utf-8')
            augmented_rows = temp_df.to_dict('records')
            print(f"üìÇ ƒê√£ load {len(augmented_rows)} m·∫´u t·ª´ file t·∫°m th·ªùi")
    
    # T·∫°o list c√°c row augmented
    variant_counter = len(augmented_rows) + 1
    
    print(f"\nüöÄ B·∫Øt ƒë·∫ßu augmentation t·ª´ index {start_index}...")
    
    try:
        for idx in range(start_index, len(df)):
            row = df.iloc[idx]
            
            # Hi·ªÉn th·ªã ti·∫øn tr√¨nh m·ªói 100 m·∫´u
            if idx % 100 == 0:
                print(f"üìä ƒêang x·ª≠ l√Ω m·∫´u {idx}/{len(df)}... (ƒë√£ t·∫°o {len(augmented_rows)} m·∫´u)")
            
            # Lu√¥n gi·ªØ m·∫´u g·ªëc
            augmented_rows.append(row)
            
            # T·∫°o th√™m phi√™n b·∫£n augmented d·ª±a tr√™n factor
            num_augs = int(augmentation_factor) + (1 if random.random() < (augmentation_factor % 1) else 0)
            
            for aug_type in range(num_augs):
                try:
                    # T·∫°o variant t·ª´ Hoi_thoai_mo_rong (n·∫øu c√≥)
                    if pd.notna(row['Hoi_thoai_mo_rong']) and row['Hoi_thoai_mo_rong'].strip():
                        aug_row_extended = await augment_example(row, variant_counter, use_extended=True)
                        augmented_rows.append(aug_row_extended)
                        variant_counter += 1
                    
                    # T·∫°o variant t·ª´ H·ªôi tho·∫°i (lu√¥n c√≥)
                    aug_row_basic = await augment_example(row, variant_counter, use_extended=False)
                    augmented_rows.append(aug_row_basic)
                    variant_counter += 1
                    
                except Exception as e:
                    print(f"‚ùå L·ªói khi augment m·∫´u {idx}: {e}")
                    raise e  # N√©m l·ªói ƒë·ªÉ d·ª´ng ch∆∞∆°ng tr√¨nh
            
            # L∆∞u checkpoint v√† file t·∫°m th·ªùi m·ªói batch_size m·∫´u
            if (idx + 1) % batch_size == 0:
                save_checkpoint(checkpoint_file, idx + 1, len(augmented_rows), augmented_rows)
                temp_file = f"temp_augmented_{idx + 1}.csv"
                save_temp_file(temp_file, augmented_rows)
        
        print(f"‚úÖ Ho√†n th√†nh augmentation!")
        
        # T·∫°o dataframe m·ªõi t·ª´ list rows
        augmented_df = pd.DataFrame(augmented_rows)
        
        # Shuffle ƒë·ªÉ tr√°nh bias
        print("üîÑ ƒêang shuffle d·ªØ li·ªáu...")
        augmented_df = augmented_df.sample(frac=1).reset_index(drop=True)
        
        # L∆∞u file output cu·ªëi c√πng
        print("üíæ ƒêang l∆∞u file cu·ªëi c√πng...")
        augmented_df.to_csv(output_file, index=False, encoding='utf-8-sig')
        
        # X√≥a checkpoint v√† file t·∫°m th·ªùi
        if os.path.exists(checkpoint_file):
            os.remove(checkpoint_file)
        
        # X√≥a c√°c file t·∫°m th·ªùi
        for i in range(0, len(df), batch_size):
            temp_file = f"temp_augmented_{i}.csv"
            if os.path.exists(temp_file):
                os.remove(temp_file)
        
        print(f"‚úÖ S·ªë l∆∞·ª£ng m·∫´u sau augment: {len(augmented_df)}")
        print(f"‚úÖ File augmented l∆∞u t·∫°i: {output_file}")
        
        # Th·ªëng k√™ sau augment
        print(f"\nüìä Th·ªëng k√™ sau augmentation:")
        print(f"- T·ªïng s·ªë m·∫´u: {len(augmented_df)}")
        print(f"- S·ªë m·∫´u g·ªëc: {len(df)}")
        print(f"- S·ªë m·∫´u ƒë∆∞·ª£c t·∫°o th√™m: {len(augmented_df) - len(df)}")
        
    except Exception as e:
        print(f"\n‚ùå L·ªói x·∫£y ra t·∫°i index {idx}: {e}")
        print(f"üíæ ƒêang l∆∞u checkpoint v√† file t·∫°m th·ªùi...")
        
        # L∆∞u checkpoint khi g·∫∑p l·ªói
        save_checkpoint(checkpoint_file, idx, len(augmented_rows), augmented_rows)
        
        # L∆∞u file t·∫°m th·ªùi
        temp_file = f"temp_augmented_{idx}.csv"
        save_temp_file(temp_file, augmented_rows)
        
        print(f"üìã ƒê·ªÉ ti·∫øp t·ª•c, ch·∫°y l·∫°i script v·ªõi c√πng tham s·ªë")
        print(f"üìÅ Checkpoint: {checkpoint_file}")
        print(f"üìÅ File t·∫°m th·ªùi: {temp_file}")
        
        raise e

# Ch·∫°y main function
if __name__ == "__main__":
    input_file = 'expanded_scam_all_types.csv'
    output_file = 'augmented_expanded_scam_dataset.csv'
    augmentation_factor = 1.0
    batch_size = 100  # L∆∞u checkpoint m·ªói 100 m·∫´u
    checkpoint_file = 'augmentation_checkpoint.json'
    
    print("üöÄ B·∫Øt ƒë·∫ßu ch·∫°y Data Augmentation v·ªõi Checkpoint...")
    print(f"üìÅ Input: {input_file}")
    print(f"üìÅ Output: {output_file}")
    print(f"üî¢ Augmentation factor: {augmentation_factor}")
    print(f"üì¶ Batch size: {batch_size}")
    print(f"üìã Checkpoint file: {checkpoint_file}")
    print("-" * 60)
    
    try:
        asyncio.run(augment_dataset_with_checkpoint(input_file, output_file, augmentation_factor, batch_size, checkpoint_file))
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è  Ch∆∞∆°ng tr√¨nh b·ªã d·ª´ng b·ªüi ng∆∞·ªùi d√πng")
        print("üíæ Checkpoint ƒë√£ ƒë∆∞·ª£c l∆∞u, b·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i ƒë·ªÉ ti·∫øp t·ª•c")
    except Exception as e:
        print(f"\n‚ùå L·ªói kh√¥ng mong mu·ªën: {e}")
        print("üíæ Checkpoint ƒë√£ ƒë∆∞·ª£c l∆∞u, b·∫°n c√≥ th·ªÉ ch·∫°y l·∫°i ƒë·ªÉ ti·∫øp t·ª•c")